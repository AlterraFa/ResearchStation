{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11601177,"sourceType":"datasetVersion","datasetId":7275824},{"sourceId":11627688,"sourceType":"datasetVersion","datasetId":7295094}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## INIT ALL CLASSES FOR FIXMATCH","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as T\nfrom torch.utils.data import (DataLoader, TensorDataset, Dataset, ConcatDataset)\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n\nfrom torch.utils.tensorboard import SummaryWriter\nfrom itertools import cycle\nfrom typing import Tuple\nfrom tqdm.auto import tqdm\n\nclass EarlyStopping:\n    def __init__(self, \n                 patience: int = 5, \n                 min_delta: float = 0.0, \n                 path: str = \"checkpoint.pt\",\n                 verbose: bool = False):\n        self.patience  = patience\n        self.min_delta = min_delta\n        self.path      = path\n        self.verbose   = verbose\n        self.counter   = 0\n        self.best_loss = torch.inf\n        self.early_stop = False\n\n    def __call__(self, val_loss: float, model: torch.nn.Module):\n        # check if loss improved by at least min_delta\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter   = 0\n            torch.save(model.state_dict(), self.path)\n            if self.verbose:\n                print(f\"Validation loss improved to {val_loss:.4f}. Saved model to {self.path}\")\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"No improvement in val loss for {self.counter}/{self.patience} epochs.\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int, dropout: float = 0.0):\n        super(ResnetBlock, self).__init__()\n        \n        self.dropout = dropout\n        \n        self.bn1 = nn.BatchNorm2d(in_channels, momentum = 0.01)\n        self.LeakyReLU1 = nn.LeakyReLU(inplace = True, negative_slope = 0.01)\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels, momentum = 0.01)\n        self.LeakyReLU2 = nn.LeakyReLU(inplace = True, negative_slope = 0.01)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n        self.InIsOut = in_channels == out_channels\n        self.shortcutCompat = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, padding = 0, bias = False) if not self.InIsOut else nn.Identity() \n        \n    def forward(self, x):\n        if not self.InIsOut:\n            x = self.LeakyReLU1(self.bn1(x))\n        else:\n            out = self.LeakyReLU1(self.bn1(x))\n\n        \n        out = self.LeakyReLU2(self.bn2(self.conv1(out if self.InIsOut else x)))\n        if self.dropout > 0:\n            out = F.dropout(out, self.dropout, training = self.training)\n            \n        out = self.conv2(out)\n        \n        return self.shortcutCompat(x) + out\n\nclass BlockStack(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int, dropout: float, numBlock: int):\n        super(BlockStack, self).__init__() \n        self.group = self.make_group(in_channels, out_channels, stride, dropout, numBlock)\n        \n    def make_group(self, in_channels: int, out_channels: int, stride: int, dropout: float, numBlock: int):\n        layers = []\n        for idx in range(numBlock):\n            if idx == 0:\n                layers.append(ResnetBlock(in_channels, out_channels, stride, dropout))\n            else:\n                layers.append(ResnetBlock(out_channels, out_channels, 1, dropout))\n        \n        return nn.Sequential(*layers)\n    def forward(self, x):\n        return self.group(x)\n\n        \n\nclass WRN(nn.Module):\n    def __init__(self, depth: int, widenFact: int, numClasses: int, dropout: float = 0.0):\n        super(WRN, self).__init__()\n        assert (depth - 4) % 6 == 0\n        numBlock = (depth - 4) // 6\n\n        channelDepth = [16, 16 * widenFact, 32 * widenFact, 64 * widenFact]\n        strides = [1, 2, 2]\n\n        self.stem = nn.Conv2d(3, channelDepth[0], kernel_size = 3, stride = 1)\n        \n        self.largeGroup = nn.ModuleList(\n            [BlockStack(channelDepth[i], channelDepth[i + 1], strides[i], dropout, numBlock) for i in range(3)]\n        )\n\n        self.bn = nn.BatchNorm2d(channelDepth[-1], momentum = 0.01)\n        self.LeakyReLU = nn.LeakyReLU(inplace = True, negative_slope = 0.01)\n        self.fc = nn.Linear(channelDepth[-1], numClasses)\n\n        \n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n        \n        \n    def forward(self, x):\n        x = self.stem(x)\n        for group in self.largeGroup:\n            x = group(x)\n        x = self.LeakyReLU(self.bn(x))\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = torch.flatten(x, 1)\n        return self.fc(x)\n\n    def summary(self):\n        \n        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total_MB = total_params * 4 / (1024 ** 2)  # Assuming 32-bit float = 4 bytes\n        print(f\"Total Trainable Parameters: {total_params:,}\")\n        print(f\"Approximate Model Size: {total_MB:.2f} MB\")\n\n\nclass TensorImageDataset(Dataset):\n    def __init__(self, images: torch.Tensor, labels: torch.Tensor = None, transform=None):\n        if labels is not None:\n            assert images.shape[0] == labels.shape[0] \n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return self.images.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.images[idx]        # shape [C,H,W]\n        if self.labels is not None:\n            y = self.labels[idx]        # oneâ€‘hot float\n        if self.transform:\n            x = self.transform(x)\n        return x, y if self.labels is not None else x\n    \nclass UnlabeledDataset(Dataset):\n    def __init__(self, images: torch.Tensor, weak_transform=None, strong_transform=None):\n        self.images = images\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n\n    def __len__(self):\n        return self.images.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.images[idx]  # shape [C, H, W]\n        x_weak = self.weak_transform(x) if self.weak_transform else x\n        x_strong = self.strong_transform(x) if self.strong_transform else x\n        return x_weak, x_strong\n\nclass DistributionAlignment(nn.Module):\n    def __init__(self, labels: torch.tensor, numClasses: int, momentum: float):\n        super(DistributionAlignment, self).__init__()\n        \n        counts = torch.bincount(labels)\n        pEmperical = (counts.float() / labels.numel())\n        \n        self.register_buffer(\"pEmperical\", pEmperical)\n        self.register_buffer(\"pRunning\", torch.zeros(numClasses))\n        self.momentum = momentum\n        \n    def forward(self, q: torch.Tensor):\n        pBatch = q.mean(dim = 0)\n        self.pRunning = (\n            self.momentum * self.pRunning \n            + (1 - self.momentum) * pBatch\n        )\n        \n        labelTilde = q * (self.pEmperical / (self.pRunning + 1e-6)).unsqueeze(0)\n        \n        return labelTilde / labelTilde.sum(dim = 1, keepdim = True)\n\ndef allSetAugment(\n    train: Tuple[torch.Tensor, torch.Tensor],\n    test: Tuple[torch.Tensor, torch.Tensor],\n    unlabeled: torch.Tensor,\n    batchSize: int,\n    muy: float,\n    splitRatio: float = 0.1\n):\n    train_images, train_labels = train\n    num_train = train_images.shape[0]\n    split = int(num_train * splitRatio)\n\n    # split off a validation slice\n    trainDS = TensorImageDataset(\n        train_images[split:], \n        train_labels[split:].long(),\n        transform=weakAugment             # or weakAugment if you want augment on train\n    )\n    valDS   = TensorImageDataset(\n        train_images[:split],\n        train_labels[:split].long(),\n        transform=None\n    )\n    testDS  = TensorImageDataset(\n        test[0],\n        test[1].long(),\n        transform=None\n    )\n    unlabeledDS = TensorImageDataset(\n        unlabeled\n    )\n\n    trainLoader     = DataLoader(trainDS, batch_size=batchSize, shuffle=True)\n    valLoader       = DataLoader(valDS,   batch_size=batchSize, shuffle=True)\n    testLoader      = DataLoader(testDS,  batch_size=batchSize, shuffle=False)\n    unlabeledLoader = DataLoader(\n        unlabeledDS, \n        batch_size=int(muy * batchSize), \n        shuffle=True\n    )\n\n    return trainLoader, valLoader, testLoader, unlabeledLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:01:04.492878Z","iopub.execute_input":"2025-05-01T01:01:04.493155Z","iopub.status.idle":"2025-05-01T01:01:25.636468Z","shell.execute_reply.started":"2025-05-01T01:01:04.493132Z","shell.execute_reply":"2025-05-01T01:01:25.635696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading dataset and intialize cuda device","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda')\ntorch.manual_seed(45)\n\nuseRatio = 1\nunlabeled = torch.load(\"/kaggle/input/stl-10/unlabeled.pt\"); unlabeled = unlabeled[: int(unlabeled.shape[0] * useRatio)].permute(0, 3, 1, 2)\ntrain = torch.load(\"/kaggle/input/stl-10/train.pt\"); trainX = train[0].to(torch.float32).permute(0, 3, 1, 2); trainY = train[1].long()\ntest = torch.load(\"/kaggle/input/stl-10/test.pt\"); testX = test[0].to(torch.float32).permute(0, 3, 1, 2); testY = test[1].long()\ntrainX = trainX / 255\ntestX = testX / 255\nunlabeled = unlabeled / 255\nnumClasses = len(torch.unique(testY))\ndel train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:01:25.637705Z","iopub.execute_input":"2025-05-01T01:01:25.638234Z","iopub.status.idle":"2025-05-01T01:01:55.559095Z","shell.execute_reply.started":"2025-05-01T01:01:25.638213Z","shell.execute_reply":"2025-05-01T01:01:55.558465Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the WRN model","metadata":{}},{"cell_type":"code","source":"writer = SummaryWriter(log_dir = \"/kaggle/working/FixMatchExperiment\")\ndepth = 40; width = 2\nmodel = WRN(depth, width, 10, dropout = 0.22)\nmodel(trainX[:1])\nmodel.summary()\nmodel.to(device)\nwriter.add_graph(model, trainX[:1].to(device))\nwriter.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:01:55.559759Z","iopub.execute_input":"2025-05-01T01:01:55.559985Z","iopub.status.idle":"2025-05-01T01:01:58.237141Z","shell.execute_reply.started":"2025-05-01T01:01:55.559967Z","shell.execute_reply":"2025-05-01T01:01:58.236264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Augment train and unlabeled dataset","metadata":{}},{"cell_type":"code","source":"weakAugment = T.Compose([\n    T.ToPILImage(),\n    T.RandomHorizontalFlip(),\n    T.ToTensor()\n])\n\nstrongAugment = T.Compose([\n    T.ToPILImage(),\n    T.RandomHorizontalFlip(),\n    T.RandAugment(num_ops = 3, magnitude = 10),\n    T.ToTensor()\n])\n\n\ntrainLoader, valLoader, testLoader, unlabeledLoader = allSetAugment(\n    [trainX, trainY], \n    [testX, testY], \n    unlabeled, \n    batchSize = 64, \n    muy = 1.4, \n    splitRatio = 0.15\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:01:58.238553Z","iopub.execute_input":"2025-05-01T01:01:58.238791Z","iopub.status.idle":"2025-05-01T01:01:58.243586Z","shell.execute_reply.started":"2025-05-01T01:01:58.238772Z","shell.execute_reply":"2025-05-01T01:01:58.243070Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TRAINING TIME","metadata":{}},{"cell_type":"code","source":"initLR = 1e-3; epochs = 480; tau = 0.55; l1 = 1e-4; l2 = 1e-3; reAugmentApply = 2; \n    \noptimizer             = optim.SGD(model.parameters(), lr = 1e-3, momentum = 0.9, nesterov = True)\nscheduler             = CosineAnnealingLR(optimizer = optimizer, T_max = 20, eta_min = 0)\nsupervisedCriterion   = nn.CrossEntropyLoss(label_smoothing = 0.1)\nunsupervisedCriterion = nn.CrossEntropyLoss(reduction = 'none')\nalignment             = DistributionAlignment(trainY, numClasses = numClasses, momentum = 0.999).to(device)\nearlystop             = EarlyStopping(50, 0.00000001, path = f\"/kaggle/working/Resnet_{depth}_{width}.pt\", verbose = True)\n\n\npbar = tqdm(range(epochs), desc=\"Training Epochs\")\nfor epoch in pbar:\n    model.train()\n    \n    supervisedCost = 0\n    consistencyCost = 0\n    totalCost = 0\n    trainCount = 0\n    counter = 0\n    for (xBatch, yBatch), unlabeled in zip(trainLoader, cycle(unlabeledLoader)):\n        unlabeled = unlabeled[0]\n        optimizer.zero_grad()\n\n        logits         = model(xBatch.to(device))\n        supervisedLoss = supervisedCriterion(logits, yBatch.to(device)).mean()\n        distribution   = torch.softmax(logits, dim = 1)\n        trainCount     += (torch.argmax(distribution, dim = 1) == yBatch.to(device)).sum().item(); counter += yBatch.shape[0]\n        del xBatch, yBatch, logits \n\n        with torch.no_grad():\n            unlabeledWeak = torch.stack([\n                weakAugment(img) for img in unlabeled\n            ])\n            wLogits            = model(unlabeledWeak.to(device))\n            qWeak              = torch.softmax(wLogits, dim = 1)\n            confs, pseudoLabel = qWeak.max(dim = 1)\n            # pseudoLabel        = pseudoLabel.detach()\n            pseudoLabel        = alignment(qWeak)\n            mask               = (confs >= tau).float()\n            del unlabeledWeak, wLogits, qWeak, confs  \n\n        unsupervisedLosses = 0.0\n        for _ in range(reAugmentApply):\n            unlabeledStrong    = torch.stack([strongAugment(img) for img in unlabeled])\n            unlabeledStrong    = unlabeledStrong.to(device)\n            sLogits            = model(unlabeledStrong)\n            scalarLoss         = (mask * unsupervisedCriterion(sLogits, pseudoLabel)).mean()\n            unsupervisedLosses += scalarLoss\n            del sLogits, unlabeledStrong\n        \n        consistencyLoss = unsupervisedLosses / reAugmentApply\n\n        weightParams = [p for n, p in model.named_parameters()\n                        if p.requires_grad and \"weight\" in n]\n        l1Norm = sum(p.abs().sum() for p in weightParams)\n        l2Norm = sum(p.pow(2.0).sum() for p in weightParams)\n        \n        loss = supervisedLoss \\\n                + consistencyLoss \\\n                + l1Norm * l1 \\\n                + l2Norm * l2\n        loss.backward()\n        optimizer.step()\n\n\n        supervisedCost  += supervisedLoss.item()\n        consistencyCost += consistencyLoss.item()\n        totalCost       += loss.item()\n    \n    trainSupLossTotal    = supervisedCost / len(trainLoader)\n    consistencyLossTotal = consistencyCost / len(trainLoader)\n    totalLoss            = totalCost / len(trainLoader)\n    trainAcc             = trainCount / counter\n\n    model.eval()\n    runningLoss = 0.0; valCount = 0; counter = 0\n    with torch.no_grad():\n        for xBatch, yBatch in valLoader:\n            xBatch = xBatch.to(device); yBatch = yBatch.to(device)\n\n            outputs      = model(xBatch)\n            loss         = supervisedCriterion(outputs, yBatch)\n            distribution = torch.softmax(outputs, dim = 1)\n\n            valCount    += (torch.argmax(distribution, dim = 1) == yBatch).sum().item()\n            counter     += yBatch.shape[0]\n            runningLoss += loss.item()\n\n    valLossTotal = runningLoss / len(valLoader)\n    valAcc = valCount / counter\n\n    scheduler.step()\n    currentLr = optimizer.param_groups[0]['lr']\n    \n    used     = torch.cuda.memory_allocated()  / 2**20\n    reserved = torch.cuda.memory_reserved()   / 2**20\n\n    \n    tqdm.write(f\"Epoch: {epoch + 1}, Supervised Loss: {trainSupLossTotal:.4f}, Consistency Loss: {consistencyLossTotal:.4f}, Loss: {totalLoss:.4f}, Train Accuracy: {100 * trainAcc:.2f}%, Val loss: {valLossTotal:.4f}, Val Acc: {100 * valAcc:.2f}%\")\n    pbar.set_postfix({\n        \"Supervised Loss\": f\"{trainSupLossTotal:.4f}\",\n        \"Consistency Loss\": f\"{consistencyLossTotal:.4f}\",\n        \"Loss\": f\"{totalLoss:.4f}\",\n        \"Val Loss\": f\"{valLossTotal:.4f}\"\n    })  \n    writer.add_scalar(\"Loss/Supervised\",     trainSupLossTotal,    epoch + 1)\n    writer.add_scalar(\"Loss/Consistency\",    consistencyLossTotal, epoch + 1)\n    writer.add_scalar(\"Loss/Total\",          totalLoss,            epoch + 1)\n    writer.add_scalar(\"Accuracy/Train\",      100 * trainAcc,       epoch + 1)\n    writer.add_scalar(\"Loss/Validation\",     valLossTotal,         epoch + 1)\n    writer.add_scalar(\"Accuracy/Validation\", 100 * valAcc,         epoch + 1)\n    writer.add_scalar(\"Misc/Lr\",             currentLr,            epoch + 1)\n    writer.add_scalar(\"Misc/GPU-used\",       used,                 epoch + 1)\n    writer.add_scalar(\"Misc/GPU-reserved\",   reserved,             epoch + 1)\n    writer.flush()\n    \n    earlystop(valLossTotal, model)\n    if earlystop.early_stop:\n        print(f\"STOPPED AT EPOCH {epoch}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:01:58.244395Z","iopub.execute_input":"2025-05-01T01:01:58.244683Z","execution_failed":"2025-05-01T01:02:23.405Z"}},"outputs":[],"execution_count":null}]}